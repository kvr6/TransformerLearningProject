# TransformerLearningProject
This repository contains a learning project where I explore the use of transformer-based models (specifically, MPNet) to generate semantic embeddings for search queries. This implementation uses PyTorch and PyTorch Lightning. 
### Description and Objectives
This project explores the application of ***all-mpnet-base-v2*** transformer model from the Hugging Face Transformers library to generate semantic embeddings for search queires. The implementation uses Python, PyTorch, and PyTorch Lightning, providing a streamlined interface for model training and deployment.

These were the learning objectives:
1. Gain a better understanding of how to utilize transformer-based models, to generate semantic embeddings for search queries.
2. **Application in Semantic Search and Recommendation Systems**: Learn more about how embeddings are generated and how they fit into the broaded context of building recommendation systems. For example, the embeddings can help systems better understand the nuances of user queries and content descriptions, allowing for more accurate recommendations of products, services, or content based on semantic similarity. This is critical for enhancing customer experience.
    
### Code walkthrough (and steps followed)

1. Install necessary Python packages - PyTorch, PyTorch Lightning, Sentence Transformers

   `pip install torch pytorch_lightning sentence_transformers`

2. After importing the required libraries, I created a `SimpleDataset` class that inherits from `torch.utils.data.Dataset`. This class is designed to handle loading and batching of data.

       class SimpleDataset(Dataset):
        def __init__(self, data):
          self.data = data

        def __len__(self):
          return len(self.data)

        def __getitem__(self, idx):
          query, description = self.data[idx]
          return {"query": query, "description": description}

   - `__init__`: Initializes the dataset with provided data
   - `__len__`: Returns the size of the dataset
   - `__getitem__`: Retreives an item by index, returning it as a dictionary of query and description pairs.
  
3. After this, I created a `SimpleDataModule` class using PyTorch Lightning's `LightningDataModule` for organizing data-related operations.
   
        class SimpleDataModule(pl.LightningDataModule):
          def __init__(self, dataset, batch_size=2):
            super().__init__()
            self.dataset = dataset
            self.batch_size = batch_size

          def train_dataloader(self):
            return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)

   This module manages data loading specifics such as batching and shuffling, facilitating easy integration and batch processing during model training. 

4. After this I defined model by creating the `SBERTEmbedder` class that encapsulates the model logic using PyTorch Lightning's module system.

       class SBERTEmbedder(pl.LightningModule):
        def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2'):
          super().__init__()
          self.model = SentenceTransformer(model_name)

        def forward(self, input_texts):
          embeddings = self.model.encode(input_texts, convert_to_tensor=True, show_progress_bar=False)
          return embeddings.requires_grad_()

        def training_step(self, batch, batch_idx):
          query_embeddings = self.forward(batch['query'])
          description_embeddings = self.forward(batch['description'])
          loss = torch.nn.functional.mse_loss(query_embeddings, description_embeddings)
          self.log('train_loss', loss)
          return loss

        def configure_optimizers(self):
          return torch.optim.Adam(self.parameters(), lr=1e-5)

        def train_model(self, data_module, max_epochs=1):
          trainer = Trainer(max_epochs=max_epochs, accelerator=self.accelerator, devices=self.devices)
          trainer.fit(self, data_module)

     - Model Initialization: Loads the `all-mpnet-base-v2` model
     - Forward Pass: Generates embeddings for given input texts
     - Training Step: Computes mean squared error between query and description embeddings as a loss to optimize
     - Optimizer Configuration: Sets up the optimizer for training
     - Model Training Function: Handles the actual model training using PyTorch Lightning's `Trainer` 
  
  5. After this, the script evaluates the model to generate embeddings for new queries and prints them.

         model.eval()  # Set the model to evaluation mode
         sample_data = ["high end water bottle", "smart home speaker", "compact wireless keyboard"]
         with torch.no_grad():  # Disable gradient computation
           sample_embeddings = model(sample_data)
         print("Sample Query Embeddings:", sample_embeddings)

  6. The `sample_embeddings.pt` file contains the tensor output of the embedding model for a few sample queries. This tensor can be loaded using PyTorch's `torch.load` function to view the 
     embeddings. Hereâ€™s how you can load and view the output embeddings generated:

         import torch
         embeddings = torch.load('sample_embeddings.pt')
         print(embeddings)

  7. Understanding the output embeddings

      - **Vector Representation**: Each embedding is a dense vector with multiple dimensions. These numbers are features learned by the model to represent different aspects of the semantic                                     content of the input text. High-dimensional embeddings capture complex patterns in the data that simpler, lower-dimensional encodings (like one-hot 
                                   encoding) cannot.
      - **Semantic Similarity**: Similarity between two embeddings could be computed via metrics like cosine similarity, Euclidean distance, or other distance metrics. Cosine similarity is 
                                 particularly popular because it measures the cosine of the angle between the two vectors, effectively normalizing the magnitude and focusing purely on the 
                                 direction.
      - **Practical Application in the context of recommendation systems**:

        For a given product - <br>
        a. Compute the cosine similarity between its embedding and the embeddings of all other products in the dataset. <br>
        b. Order these products based on their similarity scores. <br>
        c. Suggest the top N products with the highest similarity scores to the user.

 ### Potential enhancements and next steps ###

  1. **Training data expansion**: Since this was a simple learning project, the mock data was limited to 5 records. To improve model reliability, we could improve the size of the training 
                                  data and use more comprehensive model evaluation techniques.
  2. **Loss function exploration**: Experiment with different loss functions to see how they affect model performance. Here we use mean squared error, but we can do better and try things 
                                    like cosine similarity loss for embedding alignment, or contrastive and triplet loss for relative distance learning in the embedding space.
  3. **Feature engineering**: We can explore more detailed feature extraction from the product descriptions. This could involve named entity recognition to extract brands, colors, and other 
                              attributes that might be useful for refining the embeddings.
  4. **Model experimentation**: We could experiment with different transformer architectures or fine-tuning options to enhance embedding quality. We could maybe use models pre-trained on 
                                similar types of data or fine-tuning the model on a task-specific corpus.  
 





